{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TT_Blip_architecture.classifier_layer import ClsfLayer\n",
    "from TT_Blip_architecture.feature_extraction_layer import FeatureExtractionLayer\n",
    "from TT_Blip_architecture.fusion_layer import FusionLayer\n",
    "from TT_Blip_architecture.tt_blip import TT_Blip\n",
    "from TT_Blip_architecture.data_processor import DataProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfe21108bcb64d4dbf0e1b2072255e5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43f63a62bb0d495c978799b078cdb0de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import csv\n",
    "from tqdm.auto import tqdm\n",
    "import urllib.request\n",
    "\n",
    "\n",
    "train_set = []\n",
    "validation_set = []\n",
    "with open(\"Fakeddit/multimodal_train.tsv\") as fd:\n",
    "    rd = csv.DictReader(fd, delimiter=\"\\t\", fieldnames=['author','clean_title','created_utc','domain','hasImage','id','image_url','linked_submission_id','num_comments','score','subreddit','title','upvote_ratio','2_way_label','3_way_label','6_way_label'])\n",
    "    \n",
    "    for line in tqdm(rd):\n",
    "        train_set.append(line)\n",
    "    train_set = train_set[1:]\n",
    "\n",
    "with open(\"Fakeddit/multimodal_validate.tsv\") as fd:\n",
    "    rd = csv.DictReader(fd, delimiter=\"\\t\", fieldnames=['author','clean_title','created_utc','domain','hasImage','id','image_url','linked_submission_id','num_comments','score','subreddit','title','upvote_ratio','2_way_label','3_way_label','6_way_label'])\n",
    "    \n",
    "    for line in tqdm(rd):\n",
    "        validation_set.append(line)\n",
    "    validation_set = validation_set[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "processor = DataProcessor()\n",
    "\n",
    "def download_and_process_image(b):\n",
    "    image_url = b['image_url']\n",
    "    try:\n",
    "        # Download the image data into memory\n",
    "        with urllib.request.urlopen(image_url) as url_response:\n",
    "            image_data = url_response.read()\n",
    "            # Convert the byte data to a NumPy array\n",
    "            image_array = np.asarray(bytearray(image_data), dtype=np.uint8)\n",
    "            # Decode the image\n",
    "            img = cv.imdecode(image_array, cv.IMREAD_COLOR)\n",
    "            # Convert color space to RGB\n",
    "            img = cv.cvtColor(img, cv.COLOR_BGR2RGB)\n",
    "            # Get label and title\n",
    "            y = int(b['2_way_label'])\n",
    "            t = b['clean_title']\n",
    "            return img, y, t\n",
    "    except:\n",
    "        # If any error occurs, return None\n",
    "        return None\n",
    "\n",
    "def collate_fn(batch):\n",
    "    x = []\n",
    "    y = []\n",
    "    t = []\n",
    "    # Use ThreadPoolExecutor to download and process images in parallel\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        futures = [executor.submit(download_and_process_image, b) for b in batch]\n",
    "        for future in futures:\n",
    "            result = future.result()\n",
    "            if result is not None:\n",
    "                img, label, title = result\n",
    "                x.append(img)\n",
    "                y.append(label)\n",
    "                t.append(title)\n",
    "\n",
    "    if(len(x) > 0):\n",
    "        collate_fn.last_batch = (x, t, y)\n",
    "        return processor(x, t, y)\n",
    "    else:\n",
    "        (x, y, t) = collate_fn.last_batch\n",
    "        return processor(x, t, y)\n",
    "    \n",
    "collate_fn.last_batch = (0, 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[ 0.3961,  0.3804,  0.2784,  ..., -0.8824, -0.7882, -0.2392],\n",
       "           [ 0.3882,  0.4118,  0.4353,  ..., -0.8275, -0.7333, -0.0510],\n",
       "           [ 0.4275,  0.4824,  0.6000,  ..., -0.7569, -0.5686,  0.0980],\n",
       "           ...,\n",
       "           [ 0.0353,  0.1608,  0.3255,  ...,  0.8510,  0.7647,  0.8980],\n",
       "           [-0.0902,  0.1059,  0.2471,  ...,  0.8745,  0.8039,  0.9137],\n",
       "           [-0.0039,  0.2078,  0.2784,  ...,  0.8353,  0.8431,  0.9216]],\n",
       " \n",
       "          [[-0.2863, -0.2471, -0.3020,  ..., -0.9765, -0.9373, -0.5059],\n",
       "           [-0.2784, -0.1843, -0.1216,  ..., -0.9765, -0.9216, -0.3647],\n",
       "           [-0.2235, -0.1294,  0.0353,  ..., -0.9451, -0.8196, -0.2627],\n",
       "           ...,\n",
       "           [-0.5922, -0.4824, -0.3412,  ..., -0.0824, -0.1686, -0.0353],\n",
       "           [-0.7176, -0.5373, -0.4118,  ..., -0.0824, -0.1608, -0.0510],\n",
       "           [-0.6471, -0.4353, -0.3804,  ..., -0.1373, -0.1529, -0.0667]],\n",
       " \n",
       "          [[-0.6078, -0.5529, -0.6235,  ..., -0.9529, -0.9451, -0.5765],\n",
       "           [-0.6235, -0.5294, -0.4588,  ..., -0.9765, -0.9451, -0.4510],\n",
       "           [-0.6000, -0.4902, -0.3098,  ..., -0.9608, -0.8588, -0.3647],\n",
       "           ...,\n",
       "           [-0.8353, -0.7647, -0.6471,  ..., -0.5216, -0.6157, -0.5059],\n",
       "           [-0.9373, -0.8118, -0.7255,  ..., -0.5137, -0.5922, -0.5059],\n",
       "           [-0.8431, -0.7020, -0.6941,  ..., -0.5686, -0.5686, -0.4902]]],\n",
       " \n",
       " \n",
       "         [[[ 0.6941,  0.7412,  0.5686,  ...,  0.8824,  0.8824,  0.8824],\n",
       "           [ 0.6941,  0.7176,  0.6314,  ...,  0.8824,  0.8824,  0.8824],\n",
       "           [ 0.6471,  0.6471,  0.6863,  ...,  0.8824,  0.8824,  0.8824],\n",
       "           ...,\n",
       "           [ 0.5608,  0.6314,  0.7176,  ..., -0.6471, -0.5451, -0.6000],\n",
       "           [ 0.6157,  0.6549,  0.6863,  ..., -0.4196, -0.3098, -0.4745],\n",
       "           [ 0.6314,  0.6235,  0.6471,  ..., -0.1373, -0.2549, -0.2941]],\n",
       " \n",
       "          [[ 0.6941,  0.7412,  0.5686,  ...,  0.8824,  0.8824,  0.8824],\n",
       "           [ 0.6941,  0.7176,  0.6314,  ...,  0.8824,  0.8824,  0.8824],\n",
       "           [ 0.6471,  0.6471,  0.6863,  ...,  0.8824,  0.8824,  0.8824],\n",
       "           ...,\n",
       "           [ 0.5608,  0.6314,  0.7176,  ..., -0.6471, -0.5451, -0.6000],\n",
       "           [ 0.6157,  0.6549,  0.6863,  ..., -0.4196, -0.3098, -0.4745],\n",
       "           [ 0.6314,  0.6235,  0.6471,  ..., -0.1373, -0.2549, -0.2941]],\n",
       " \n",
       "          [[ 0.6941,  0.7412,  0.5686,  ...,  0.8824,  0.8824,  0.8824],\n",
       "           [ 0.6941,  0.7176,  0.6314,  ...,  0.8824,  0.8824,  0.8824],\n",
       "           [ 0.6471,  0.6471,  0.6863,  ...,  0.8824,  0.8824,  0.8824],\n",
       "           ...,\n",
       "           [ 0.5608,  0.6314,  0.7176,  ..., -0.6471, -0.5451, -0.6000],\n",
       "           [ 0.6157,  0.6549,  0.6863,  ..., -0.4196, -0.3098, -0.4745],\n",
       "           [ 0.6314,  0.6235,  0.6471,  ..., -0.1373, -0.2549, -0.2941]]]]),\n",
       " tensor([[[[ 6.8947e-01,  7.6246e-01,  7.6246e-01,  ..., -1.5295e+00,\n",
       "            -9.8935e-01, -3.9081e-01],\n",
       "           [ 8.9385e-01,  9.2304e-01,  8.2086e-01,  ..., -1.3981e+00,\n",
       "            -5.9519e-01,  4.4130e-01],\n",
       "           [ 7.3327e-01,  7.4786e-01,  8.2086e-01,  ..., -1.4419e+00,\n",
       "            -6.5359e-01,  5.1429e-01],\n",
       "           ...,\n",
       "           [-2.1563e-01, -1.4264e-01,  1.4933e-01,  ...,  1.6238e+00,\n",
       "             1.6968e+00,  1.8427e+00],\n",
       "           [-8.4247e-02,  1.2013e-01,  4.2670e-01,  ...,  1.7260e+00,\n",
       "             1.8281e+00,  1.8865e+00],\n",
       "           [-1.1255e-02,  2.2232e-01,  5.4349e-01,  ...,  1.5362e+00,\n",
       "             1.6822e+00,  1.7698e+00]],\n",
       " \n",
       "          [[-5.3647e-01, -4.1641e-01, -3.2636e-01,  ..., -1.7071e+00,\n",
       "            -1.3919e+00, -8.2162e-01],\n",
       "           [-3.2636e-01, -2.2130e-01, -2.3631e-01,  ..., -1.6470e+00,\n",
       "            -1.0167e+00, -1.1196e-02],\n",
       "           [-4.4642e-01, -3.7138e-01, -2.2130e-01,  ..., -1.7371e+00,\n",
       "            -1.1668e+00, -2.6204e-02],\n",
       "           ...,\n",
       "           [-1.3319e+00, -1.2568e+00, -9.7169e-01,  ..., -1.0124e-01,\n",
       "            -4.1212e-02,  1.5389e-01],\n",
       "           [-1.2118e+00, -1.0017e+00, -6.8655e-01,  ..., -2.6204e-02,\n",
       "             7.8851e-02,  1.6890e-01],\n",
       "           [-1.1518e+00, -8.9665e-01, -5.8149e-01,  ..., -2.3631e-01,\n",
       "            -8.6235e-02,  3.8118e-03]],\n",
       " \n",
       "          [[-8.9720e-01, -7.6922e-01, -6.8390e-01,  ..., -1.4518e+00,\n",
       "            -1.2100e+00, -7.5500e-01],\n",
       "           [-6.9812e-01, -6.4124e-01, -6.1280e-01,  ..., -1.4091e+00,\n",
       "            -8.9720e-01, -1.3329e-03],\n",
       "           [-8.6876e-01, -8.2610e-01, -6.5546e-01,  ..., -1.4802e+00,\n",
       "            -1.0394e+00, -2.9773e-02],\n",
       "           ...,\n",
       "           [-1.4660e+00, -1.4233e+00, -1.2385e+00,  ..., -6.9812e-01,\n",
       "            -6.6968e-01, -5.2748e-01],\n",
       "           [-1.3238e+00, -1.1674e+00, -9.6830e-01,  ..., -6.1280e-01,\n",
       "            -5.1326e-01, -4.7060e-01],\n",
       "           [-1.2527e+00, -1.0678e+00, -8.5454e-01,  ..., -7.9766e-01,\n",
       "            -6.5546e-01, -5.7014e-01]]],\n",
       " \n",
       " \n",
       "         [[[ 1.3026e+00,  1.4340e+00,  1.4924e+00,  ...,  1.7114e+00,\n",
       "             1.7114e+00,  1.7114e+00],\n",
       "           [ 1.3464e+00,  1.4340e+00,  1.4778e+00,  ...,  1.7114e+00,\n",
       "             1.7114e+00,  1.7114e+00],\n",
       "           [ 1.3756e+00,  1.3756e+00,  1.4194e+00,  ...,  1.7114e+00,\n",
       "             1.7114e+00,  1.7114e+00],\n",
       "           ...,\n",
       "           [ 1.1858e+00,  1.2442e+00,  1.2880e+00,  ..., -4.2001e-01,\n",
       "            -4.9300e-01, -9.4555e-01],\n",
       "           [ 1.2150e+00,  1.2442e+00,  1.2442e+00,  ..., -4.0541e-01,\n",
       "            -4.6381e-01, -5.2220e-01],\n",
       "           [ 1.2588e+00,  1.2296e+00,  1.2150e+00,  ..., -4.4921e-01,\n",
       "            -5.8059e-01, -3.0322e-01]],\n",
       " \n",
       "          [[ 1.4295e+00,  1.5646e+00,  1.6247e+00,  ...,  1.8498e+00,\n",
       "             1.8498e+00,  1.8498e+00],\n",
       "           [ 1.4746e+00,  1.5646e+00,  1.6096e+00,  ...,  1.8498e+00,\n",
       "             1.8498e+00,  1.8498e+00],\n",
       "           [ 1.5046e+00,  1.5046e+00,  1.5496e+00,  ...,  1.8498e+00,\n",
       "             1.8498e+00,  1.8498e+00],\n",
       "           ...,\n",
       "           [ 1.3095e+00,  1.3695e+00,  1.4145e+00,  ..., -3.4137e-01,\n",
       "            -4.1641e-01, -8.8165e-01],\n",
       "           [ 1.3395e+00,  1.3695e+00,  1.3695e+00,  ..., -3.2636e-01,\n",
       "            -3.8639e-01, -4.4642e-01],\n",
       "           [ 1.3845e+00,  1.3545e+00,  1.3395e+00,  ..., -3.7138e-01,\n",
       "            -5.0645e-01, -2.2130e-01]],\n",
       " \n",
       "          [[ 1.5344e+00,  1.6624e+00,  1.7193e+00,  ...,  1.9326e+00,\n",
       "             1.9326e+00,  1.9326e+00],\n",
       "           [ 1.5771e+00,  1.6624e+00,  1.7051e+00,  ...,  1.9326e+00,\n",
       "             1.9326e+00,  1.9326e+00],\n",
       "           [ 1.6055e+00,  1.6055e+00,  1.6482e+00,  ...,  1.9326e+00,\n",
       "             1.9326e+00,  1.9326e+00],\n",
       "           ...,\n",
       "           [ 1.4207e+00,  1.4776e+00,  1.5202e+00,  ..., -1.4353e-01,\n",
       "            -2.1463e-01, -6.5546e-01],\n",
       "           [ 1.4491e+00,  1.4776e+00,  1.4776e+00,  ..., -1.2931e-01,\n",
       "            -1.8619e-01, -2.4307e-01],\n",
       "           [ 1.4918e+00,  1.4633e+00,  1.4491e+00,  ..., -1.7197e-01,\n",
       "            -2.9995e-01, -2.9773e-02]]]]),\n",
       " (tensor([[  101,  2122,  8239,  6649,  4201, 14219,   102],\n",
       "          [  101,  5604,  2374, 22674,  1999,   102,     0]]),\n",
       "  tensor([[1, 1, 1, 1, 1, 1, 1],\n",
       "          [1, 1, 1, 1, 1, 1, 0]])),\n",
       " (tensor([[  101,  2122,  8239,  6649,  4201, 14219,   102],\n",
       "          [  101,  5604,  2374, 22674,  1999,   102,     0]]),\n",
       "  tensor([[1, 1, 1, 1, 1, 1, 1],\n",
       "          [1, 1, 1, 1, 1, 1, 0]])),\n",
       " tensor([[0.],\n",
       "         [1.]]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dl = DataLoader(train_set[5000:], batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "val_dl = DataLoader(validation_set[1000:], batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "feature_extraction = FeatureExtractionLayer()\n",
    "fusion_layer = FusionLayer()\n",
    "clsf_layer = ClsfLayer()\n",
    "\n",
    "tt_blip = TT_Blip(feature_extraction, fusion_layer, clsf_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mosusume\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20250129_140724-hs1r396g</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/osusume/Thesis/runs/hs1r396g' target=\"_blank\">TT-Blip Fakeddit</a></strong> to <a href='https://wandb.ai/osusume/Thesis' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/osusume/Thesis' target=\"_blank\">https://wandb.ai/osusume/Thesis</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/osusume/Thesis/runs/hs1r396g' target=\"_blank\">https://wandb.ai/osusume/Thesis/runs/hs1r396g</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                     | Type                   | Params | Mode \n",
      "----------------------------------------------------------------------------\n",
      "0 | feature_extraction_layer | FeatureExtractionLayer | 443 M  | train\n",
      "1 | fusion_layer             | FusionLayer            | 14.2 M | train\n",
      "2 | clsf_layer               | ClsfLayer              | 5.9 M  | train\n",
      "3 | loss_fn                  | BCEWithLogitsLoss      | 0      | train\n",
      "4 | accuracy                 | BinaryAccuracy         | 0      | train\n",
      "----------------------------------------------------------------------------\n",
      "63.8 M    Trainable params\n",
      "400 M     Non-trainable params\n",
      "463 M     Total params\n",
      "1,855.217 Total estimated model params size (MB)\n",
      "154       Modules in train mode\n",
      "832       Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0efd474f1de24bc4bbe6519bb8588090",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniele/Desktop/tt_blip_implementation/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:476: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "/home/daniele/Desktop/tt_blip_implementation/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/daniele/Desktop/tt_blip_implementation/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "146885054db74d3486d0018be7095e69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from lightning import Trainer\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "\n",
    "logger = WandbLogger(project=\"Thesis\", name=\"TT-Blip Fakeddit\")\n",
    "trainer = Trainer(logger=logger, log_every_n_steps=1, max_epochs=50, accumulate_grad_batches=8)\n",
    "\n",
    "trainer.fit(tt_blip, train_dl, val_dl)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
