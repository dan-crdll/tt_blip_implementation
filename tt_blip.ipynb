{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n"
     ]
    }
   ],
   "source": [
    "from TT_BLIP.batch_extractor import DatasetLoader\n",
    "\n",
    "ds_loader = DatasetLoader(batch_size=1, balance=True)\n",
    "train_dl, val_dl = ds_loader.get_dataloaders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: Real 556 | Fake 528 | Naive baseline acc: 0.4870848708487085\n"
     ]
    }
   ],
   "source": [
    "ds = ds_loader.train_dataset\n",
    "\n",
    "real = 0\n",
    "fake = 0\n",
    "for e in ds:\n",
    "    if e[1]:\n",
    "        real += 1\n",
    "    else:\n",
    "        fake += 1\n",
    "\n",
    "print(f\"Train set: Real {real} | Fake {fake} | Naive baseline acc: {fake / (real + fake)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Real 122 | Fake 150 | Naive baseline acc: 0.5514705882352942\n"
     ]
    }
   ],
   "source": [
    "ds = ds_loader.test_dataset\n",
    "\n",
    "real = 0\n",
    "fake = 0\n",
    "for e in ds:\n",
    "    if e[1]:\n",
    "        real += 1\n",
    "    else:\n",
    "        fake += 1\n",
    "\n",
    "print(f\"Test set: Real {real} | Fake {fake} | Naive baseline acc: {fake / (real + fake)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TT_BLIP.tt_blip_layers import TT_BLIP_Model\n",
    "from lightning import Trainer\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "\n",
    "\n",
    "model = TT_BLIP_Model(\n",
    "        ds_loader.dp.empty_pixel_values, \n",
    "        ds_loader.dp.empty_input_ids,\n",
    "        ds_loader.dp.empty_attn_mask, \n",
    "        768, \n",
    "        8,\n",
    "        trainable=-3\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  | Name                     | Type                   | Params | Mode \n",
       "----------------------------------------------------------------------------\n",
       "0 | feature_extraction_layer | FeatureExtractionLayer | 866 M  | train\n",
       "1 | fusion_layer             | FusionLayer            | 193 M  | train\n",
       "2 | classification_layer     | ClassificationLayer    | 1.2 M  | train\n",
       "3 | loss_fn                  | BCEWithLogitsLoss      | 0      | train\n",
       "4 | acc_fn                   | BinaryAccuracy         | 0      | train\n",
       "5 | f1_fn                    | BinaryF1Score          | 0      | train\n",
       "6 | prec_fn                  | BinaryPrecision        | 0      | train\n",
       "7 | recall_fn                | BinaryRecall           | 0      | train\n",
       "----------------------------------------------------------------------------\n",
       "559 M     Trainable params\n",
       "502 M     Non-trainable params\n",
       "1.1 B     Total params\n",
       "4,247.737 Total estimated model params size (MB)\n",
       "380       Modules in train mode\n",
       "1910      Modules in eval mode"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lightning.pytorch.utilities.model_summary import ModelSummary\n",
    "\n",
    "ModelSummary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = next(iter(train_dl))\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 578, 768])\n",
      "torch.Size([1, 3, 768])\n",
      "tensor([-0.0524])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    z = model.feature_extraction_layer(*x)\n",
    "    print(z[0].shape)\n",
    "    z = model.fusion_layer(z)\n",
    "    print(z.shape)\n",
    "    y = model.classification_layer(z)\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = WandbLogger('TT_BLIP_gossipcop_balanced_256', project=\"Thesis_New\")\n",
    "trainer = Trainer(max_epochs=50, logger=logger, log_every_n_steps=1, accumulate_grad_batches=64, precision=16)\n",
    "trainer.fit(model, train_dl, val_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm \n",
    "import torch \n",
    "\n",
    "model.eval()\n",
    "\n",
    "labels = []\n",
    "preds = []\n",
    "for b in tqdm(val_dl):\n",
    "    x, y = b\n",
    "    labels.append(y)\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(x).cpu()\n",
    "        preds.append(y_pred) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = torch.zeros((2, 2))\n",
    "\n",
    "for idx in range(len(preds)):\n",
    "    i = int(labels[idx].item())\n",
    "    j = int(preds[idx].item() > 0.5)\n",
    "    cm[i, j] += 1\n",
    "\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_normalized = cm / cm.sum(axis=1, keepdim=True)\n",
    "cm_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.axis('off')\n",
    "plt.imshow(cm_normalized, cmap='Blues', vmin=0, vmax=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
