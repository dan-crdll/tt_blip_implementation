{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniele/Scrivania/tt_blip_implementation/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['id', 'text', 'image', 'fake_cls', 'fake_text_pos', 'mtcnn_boxes', 'fake_image_box'],\n",
       "     num_rows: 10409\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['id', 'text', 'image', 'fake_cls', 'fake_text_pos', 'mtcnn_boxes', 'fake_image_box'],\n",
       "     num_rows: 1106\n",
       " }))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "train_dataset = load_dataset(\"rshaojimmy/DGM4\", split=\"train[:5%]\")\n",
    "validation_dataset = load_dataset(\"rshaojimmy/DGM4\", split=\"validation[:5%]\")\n",
    "train_dataset, validation_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import cv2 as cv\n",
    "from transformers import BertTokenizerFast, AutoImageProcessor, AutoTokenizer\n",
    "\n",
    "bert_tokenizer = BertTokenizerFast.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "vit_processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "blip_txt_processor = AutoTokenizer.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "blip_img_processor = AutoImageProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "\n",
    "def process_func(batch):\n",
    "    image = []\n",
    "    text = []\n",
    "    label = []\n",
    "\n",
    "    for b in batch:\n",
    "        img = cv.imread(b['image'], cv.IMREAD_ANYCOLOR)\n",
    "        img = cv.cvtColor(img, cv.COLOR_BGR2RGB)\n",
    "        image.append(img)\n",
    "        text.append(b['text'])\n",
    "        label.append(1 if b['fake_cls'] == 'orig' else 0)\n",
    "\n",
    "    vit_img = vit_processor(image, return_tensors='pt').pixel_values\n",
    "    blip_img = blip_img_processor(image, return_tensors='pt').pixel_values\n",
    "    blip_txt = blip_txt_processor(text, return_tensors='pt', padding=True)\n",
    "    bert_txt = bert_tokenizer(text, return_tensors='pt', padding=True)\n",
    "\n",
    "    blip_tokens = blip_txt.input_ids\n",
    "    blip_attn = blip_txt.attention_mask\n",
    "\n",
    "    bert_tokens = bert_txt.input_ids\n",
    "    bert_attn = bert_txt.attention_mask\n",
    "\n",
    "\n",
    "    label = torch.tensor(label).unsqueeze(-1).float()\n",
    "    return vit_img, blip_img, (blip_tokens, blip_attn), (bert_tokens, bert_attn), label\n",
    "\n",
    "train_dl = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=process_func, drop_last=True)\n",
    "val_dl = DataLoader(validation_dataset, batch_size=8, shuffle=True, collate_fn=process_func, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 3, 224, 224]),\n",
       " torch.Size([8, 3, 384, 384]),\n",
       " torch.Size([8, 32]),\n",
       " torch.Size([8, 32]),\n",
       " torch.Size([8, 1]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ri, bi, (bt, ba), (bet, bea), lab = next(iter(train_dl))\n",
    "\n",
    "ri.shape, bi.shape, bt.shape, bea.shape, lab.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn \n",
    "import lightning as L \n",
    "from transformers import BertModel, ViTModel, BlipForConditionalGeneration\n",
    "\n",
    "\n",
    "class FeatureExtractionLayer(L.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "        self.vit = ViTModel.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "        self.blip = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "        self.bert.eval()\n",
    "        self.vit.eval()\n",
    "        self.blip.eval()\n",
    "\n",
    "        self.bert.requires_grad_(False)\n",
    "        self.vit.requires_grad_(False)\n",
    "        self.blip.requires_grad_(False)\n",
    "\n",
    "        self.vit.encoder.layer[9:].requires_grad_(True)\n",
    "        self.vit.encoder.layer[9:].train()\n",
    "        self.vit.pooler.requires_grad_(True)\n",
    "        self.vit.pooler.train()\n",
    "\n",
    "        self.bert.encoder.layer[9:].requires_grad_(True)\n",
    "        self.bert.encoder.layer[9:].train()\n",
    "        self.bert.pooler.requires_grad_(True)\n",
    "        self.bert.pooler.train()\n",
    "\n",
    "        self.dummy_img = nn.Parameter(torch.zeros((1, 3, 384, 384)), requires_grad=False)\n",
    "        self.dummy_txt = nn.Parameter(torch.zeros((1, 1), dtype=torch.int), requires_grad=False)\n",
    "        self.dummy_attn = nn.Parameter(torch.zeros((1, 1), dtype=torch.int), requires_grad=False)\n",
    "\n",
    "    def forward(self, vit_img, blip_img, blip_txt, blip_attn, bert_txt, bert_attn):\n",
    "        BSZ, *_ = vit_img.shape\n",
    "\n",
    "        # IMAGES\n",
    "        vit_encodings = self.vit(vit_img).last_hidden_state # 197 x 768\n",
    "        blip_img_encodings = self.blip(blip_img, self.dummy_txt.repeat(BSZ, 1), attention_mask=self.dummy_attn.repeat(BSZ, 1)).last_hidden_state    # 577 x 768\n",
    "        zi = torch.cat([vit_encodings, blip_img_encodings], dim=1)  # 197 + 577 x 768\n",
    "\n",
    "\n",
    "        # TEXT\n",
    "        bert_encodings = self.bert(bert_txt, bert_attn).last_hidden_state   # ?? x 768\n",
    "        blip_txt_encodings = self.blip(self.dummy_img.repeat(BSZ, 1, 1, 1), blip_txt, attention_mask=blip_attn).last_hidden_state   # 577 x 768\n",
    "        zt = torch.cat([bert_encodings, blip_txt_encodings], dim=1) # 577 + ?? x 768\n",
    "\n",
    "\n",
    "        # IMAGE / TEXT\n",
    "        blip_img_txt_encodings = self.blip(blip_img, blip_txt, attention_mask=blip_attn).last_hidden_state  # 577 x 768\n",
    "        zit = blip_img_txt_encodings\n",
    "\n",
    "        return zi, zt, zit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 774, 768]),\n",
       " torch.Size([8, 609, 768]),\n",
       " torch.Size([8, 577, 768]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_extraction_layer = FeatureExtractionLayer()\n",
    "with torch.no_grad():\n",
    "    zi, zt, zit = feature_extraction_layer(ri, bi, bt, ba, bet, bea)\n",
    "\n",
    "zi.shape, zt.shape, zit.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  | Name         | Type                         | Params | Mode\n",
       "---------------------------------------------------------------------\n",
       "0 | bert         | BertModel                    | 109 M  | eval\n",
       "1 | vit          | ViTModel                     | 86.4 M | eval\n",
       "2 | blip         | BlipForConditionalGeneration | 247 M  | eval\n",
       "  | other params | n/a                          | 442 K  | n/a \n",
       "---------------------------------------------------------------------\n",
       "43.7 M    Trainable params\n",
       "400 M     Non-trainable params\n",
       "443 M     Total params\n",
       "1,774.912 Total estimated model params size (MB)\n",
       "114       Modules in train mode\n",
       "832       Modules in eval mode"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lightning.pytorch.utilities.model_summary import ModelSummary\n",
    "\n",
    "ModelSummary(feature_extraction_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FusionLayer(L.LightningModule):\n",
    "    def __init__(self, h=4):\n",
    "        super().__init__()\n",
    "\n",
    "        self.ca_img = nn.MultiheadAttention(768, h, 0.1, batch_first=True)\n",
    "        self.ca_img_txt = nn.MultiheadAttention(768, h, 0.1, batch_first=True)\n",
    "        self.sa_txt = nn.MultiheadAttention(768, h, 0.1, batch_first=True)\n",
    "\n",
    "        self.mlp_img = nn.Sequential(\n",
    "            nn.Linear(768, 768 * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(768 * 2, 768),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.mlp_txt = nn.Sequential(\n",
    "            nn.Linear(768, 768 * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(768 * 2, 768),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.mlp_img_txt = nn.Sequential(\n",
    "            nn.Linear(768, 768 * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(768 * 2, 768),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    \n",
    "    def forward(self, zi, zt, zit):\n",
    "        zi, _ = self.ca_img(zt, zi, zi)\n",
    "        zit, _ = self.ca_img_txt(zt, zit, zit)\n",
    "        zt, _ = self.sa_txt(zt, zt, zt)\n",
    "\n",
    "        zi = self.mlp_img(zi)\n",
    "        zt = self.mlp_txt(zt)\n",
    "        zit = self.mlp_img(zit)\n",
    "\n",
    "        z = torch.cat([zi, zt, zit], dim=1)\n",
    "        return z "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 1827, 768])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fusion_layer = FusionLayer()\n",
    "\n",
    "z = fusion_layer(zi, zt, zit)\n",
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  | Name        | Type               | Params | Mode \n",
       "-----------------------------------------------------------\n",
       "0 | ca_img      | MultiheadAttention | 2.4 M  | train\n",
       "1 | ca_img_txt  | MultiheadAttention | 2.4 M  | train\n",
       "2 | sa_txt      | MultiheadAttention | 2.4 M  | train\n",
       "3 | mlp_img     | Sequential         | 2.4 M  | train\n",
       "4 | mlp_txt     | Sequential         | 2.4 M  | train\n",
       "5 | mlp_img_txt | Sequential         | 2.4 M  | train\n",
       "-----------------------------------------------------------\n",
       "14.2 M    Trainable params\n",
       "0         Non-trainable params\n",
       "14.2 M    Total params\n",
       "56.688    Total estimated model params size (MB)\n",
       "21        Modules in train mode\n",
       "0         Modules in eval mode"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ModelSummary(fusion_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F \n",
    "\n",
    "class ClsfLayer(L.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.clsf = nn.Sequential(\n",
    "            nn.Linear(768, 768 * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(768 * 2, 768 * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(768 * 2, 768 * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(768 * 2, 1),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "    \n",
    "    def forward(self, z):\n",
    "        z = self.clsf(z)\n",
    "        _, N = z.shape \n",
    "        y = F.avg_pool1d(z, N, N)\n",
    "        return y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clsf_layer = ClsfLayer()\n",
    "\n",
    "y = clsf_layer(z)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  | Name | Type       | Params | Mode \n",
       "--------------------------------------------\n",
       "0 | clsf | Sequential | 5.9 M  | train\n",
       "--------------------------------------------\n",
       "5.9 M     Trainable params\n",
       "0         Non-trainable params\n",
       "5.9 M     Total params\n",
       "23.618    Total estimated model params size (MB)\n",
       "9         Modules in train mode\n",
       "0         Modules in eval mode"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ModelSummary(clsf_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  | Name                     | Type                   | Params | Mode \n",
       "----------------------------------------------------------------------------\n",
       "0 | feature_extraction_layer | FeatureExtractionLayer | 443 M  | train\n",
       "1 | fusion_layer             | FusionLayer            | 14.2 M | train\n",
       "2 | clsf_layer               | ClsfLayer              | 5.9 M  | train\n",
       "3 | loss_fn                  | BCEWithLogitsLoss      | 0      | train\n",
       "4 | accuracy                 | BinaryAccuracy         | 0      | train\n",
       "----------------------------------------------------------------------------\n",
       "63.8 M    Trainable params\n",
       "400 M     Non-trainable params\n",
       "463 M     Total params\n",
       "1,855.217 Total estimated model params size (MB)\n",
       "149       Modules in train mode\n",
       "832       Modules in eval mode"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchmetrics import Accuracy\n",
    "\n",
    "\n",
    "class TT_Blip(L.LightningModule):\n",
    "    def __init__(self, feature_extraction_layer, fusion_layer, clsf_layer):\n",
    "        super().__init__()\n",
    "\n",
    "        self.feature_extraction_layer = feature_extraction_layer\n",
    "        self.fusion_layer = fusion_layer\n",
    "        self.clsf_layer = clsf_layer\n",
    "\n",
    "        self.loss_fn = nn.BCEWithLogitsLoss()\n",
    "        self.accuracy = Accuracy('binary')\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), 1e-3)\n",
    "    \n",
    "    def forward(self, vit_img, blip_img, blip_txt, blip_attn, bert_txt, bert_attn):\n",
    "        zi, zt, zit = self.feature_extraction_layer(vit_img, blip_img, blip_txt, blip_attn, bert_txt, bert_attn)\n",
    "        z = self.fusion_layer(zi, zt, zit)\n",
    "        y = self.clsf_layer(z)\n",
    "        return y \n",
    "    \n",
    "    def training_step(self, batch):\n",
    "        vi, bi, (bt, ba), (bet, bea), y = batch\n",
    "        pred = self.forward(vi, bi, bt, ba, bet, bea)\n",
    "\n",
    "        loss = self.loss_fn(pred, y)\n",
    "        acc = self.accuracy(pred, y)\n",
    "\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        self.log('train_acc', acc, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        return loss \n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        vi, bi, (bt, ba), (bet, bea), y = batch\n",
    "        pred = self.forward(vi, bi, bt, ba, bet, bea)\n",
    "\n",
    "        loss = self.loss_fn(pred, y)\n",
    "        acc = self.accuracy(pred, y)\n",
    "\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        self.log('val_acc', acc, prog_bar=True)\n",
    "        return loss \n",
    "\n",
    "\n",
    "model = TT_Blip(feature_extraction_layer, fusion_layer, clsf_layer)\n",
    "\n",
    "ModelSummary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mosusume\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20250119_200421-caks34tn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/osusume/Thesis/runs/caks34tn' target=\"_blank\">ViT_TT_Blip_training</a></strong> to <a href='https://wandb.ai/osusume/Thesis' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/osusume/Thesis' target=\"_blank\">https://wandb.ai/osusume/Thesis</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/osusume/Thesis/runs/caks34tn' target=\"_blank\">https://wandb.ai/osusume/Thesis/runs/caks34tn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                     | Type                   | Params | Mode \n",
      "----------------------------------------------------------------------------\n",
      "0 | feature_extraction_layer | FeatureExtractionLayer | 443 M  | train\n",
      "1 | fusion_layer             | FusionLayer            | 14.2 M | train\n",
      "2 | clsf_layer               | ClsfLayer              | 5.9 M  | train\n",
      "3 | loss_fn                  | BCEWithLogitsLoss      | 0      | train\n",
      "4 | accuracy                 | BinaryAccuracy         | 0      | train\n",
      "----------------------------------------------------------------------------\n",
      "63.8 M    Trainable params\n",
      "400 M     Non-trainable params\n",
      "463 M     Total params\n",
      "1,855.217 Total estimated model params size (MB)\n",
      "149       Modules in train mode\n",
      "832       Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniele/Scrivania/tt_blip_implementation/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:476: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "/home/daniele/Scrivania/tt_blip_implementation/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniele/Scrivania/tt_blip_implementation/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  62%|██████▏   | 810/1301 [25:23<15:23,  0.53it/s, v_num=34tn, train_loss=0.712]"
     ]
    }
   ],
   "source": [
    "from lightning.pytorch import Trainer\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "\n",
    "logger = WandbLogger(name='ViT_TT_Blip_training', project=\"Thesis\")\n",
    "trainer = Trainer(max_epochs=50, logger=logger, log_every_n_steps=1, accumulate_grad_batches=8)\n",
    "\n",
    "trainer.fit(model, train_dl, val_dl)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
